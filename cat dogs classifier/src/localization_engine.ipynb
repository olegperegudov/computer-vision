{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd0bd47a18303be29f6974e8153b6e52f9a90ee51d979b528b256bf5f0592078974",
   "display_name": "Python 3.9.2 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "bd47a18303be29f6974e8153b6e52f9a90ee51d979b528b256bf5f0592078974"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1390,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add more fc layers for 5 digit output\n",
    "# train.py\n",
    "# inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1391,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "\n",
    "# import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import ttach as tta\n",
    "\n",
    "import albumentations as A\n",
    "import albumentations.pytorch\n",
    "\n",
    "import models\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1392,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1393,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting seed\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1394,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting device to cuda if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1395,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMS HERE\n",
    "\n",
    "# params\n",
    "lr = 3e-3\n",
    "momentum = 0.9\n",
    "weight_decay = 3e-3\n",
    "\n",
    "# lr scheduler\n",
    "step_size = 4 # after this many epochs we will mult our lr by gamma\n",
    "gamma = 0.1 # lr multiplier every step_size epochs\n",
    "\n",
    "# transforms\n",
    "presize = 256\n",
    "crop = 256\n",
    "\n",
    "# batch size\n",
    "batch_size = 32\n",
    "\n",
    "# n_epochs\n",
    "frozen = 2\n",
    "unfrozen = 2\n",
    "\n",
    "# tta\n",
    "tta_crop = int(presize*0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1396,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms\n",
    "train_transform = A.Compose([\n",
    "        # A.SmallestMaxSize(presize),\n",
    "        A.RandomSizedBBoxSafeCrop (crop, crop),\n",
    "        A.RandomCrop(crop, crop),\n",
    "        A.Normalize(),\n",
    "        A.Rotate(limit=30),\n",
    "        A.HorizontalFlip(),\n",
    "        A.Cutout(),\n",
    "        A.Resize(crop, crop),\n",
    "        albumentations.pytorch.ToTensorV2()], \n",
    "        bbox_params=A.BboxParams(format='albumentations', min_area=256, min_visibility=0.1))\n",
    "\n",
    "valid_transform = A.Compose([\n",
    "        # A.SmallestMaxSize(presize),\n",
    "        A.RandomSizedBBoxSafeCrop (crop, crop),\n",
    "        A.CenterCrop(crop, crop),\n",
    "        A.Normalize(),\n",
    "        A.Resize(crop, crop),\n",
    "        albumentations.pytorch.ToTensorV2()],\n",
    "        bbox_params=A.BboxParams(format='albumentations', min_area=256, min_visibility=0.1))\n",
    "\n",
    "test_transform = A.Compose([\n",
    "        # A.SmallestMaxSize(presize),\n",
    "        A.RandomSizedBBoxSafeCrop (crop, crop),\n",
    "        A.CenterCrop(crop, crop),\n",
    "        A.Normalize(),\n",
    "        A.Resize(crop, crop),\n",
    "        albumentations.pytorch.ToTensorV2()],\n",
    "        bbox_params=A.BboxParams(format='albumentations', min_area=256, min_visibility=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1397,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset class\n",
    "class dataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.df.shape[0])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # image = Image.open(self.df.fname[index]).convert('RGB')\n",
    "        # image = np.array(image)\n",
    "        image = cv2.imread(self.df.fname[index])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        # image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # label = torch.tensor(self.df.label[index]).long()\n",
    "        class_id = self.df.label[index]\n",
    "        # assert class_id is not None, 'class id is None'\n",
    "\n",
    "        xmin = self.df.xmin_alb[index]\n",
    "        ymin = self.df.ymin_alb[index]\n",
    "        xmax = self.df.xmax_alb[index]\n",
    "        ymax = self.df.ymax_alb[index]\n",
    "\n",
    "        # assert xmin is not None, 'xmin is None'\n",
    "        # assert ymin is not None, 'ymin is None'\n",
    "        # assert xmax is not None, 'xmax is None'\n",
    "        # assert ymax is not None, 'ymax is None'\n",
    "\n",
    "        # it will need lol to itterate through\n",
    "        # bboxes = [xmin, ymin, xmax, ymax, class_id]\n",
    "        bboxes = [[xmin, ymin, xmax, ymax, class_id]]\n",
    "        # bbox.append(class_id)\n",
    "        # print(f'bbox: {bboxes}')\n",
    "        \n",
    "        # assert len([item for sublist in bboxes for item in sublist])==5, 'check error here'\n",
    "\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image, bboxes=bboxes)\n",
    "            \n",
    "            image = transformed['image']\n",
    "            bboxes = transformed['bboxes']\n",
    "\n",
    "        # print(f'transformed bbox: {bboxes}')\n",
    "        # xmin, ymin, xmax, ymax, class_id = [3,4]\n",
    "        # xmin, ymin, xmax, ymax, class_id = [item for sublist in bboxes for item in sublist]\n",
    "        # label = torch.tensor((xmin, ymin, xmax, ymax, class_id))\n",
    "        # label = torch.tensor(bboxes)\n",
    "        label = torch.tensor(bboxes).flatten()\n",
    "\n",
    "        # print(f'label shape: {label.shape}')\n",
    "\n",
    "        # if not bboxes:\n",
    "        #     label = torch.tensor([0,0,1,1,class_id])\n",
    "        # else:\n",
    "        #     label = torch.tensor(bboxes).flatten()\n",
    "\n",
    "        # print('hi')\n",
    "        # print(bbox)\n",
    "        # print(f'label len: {len(label)}')\n",
    "        # print(f'image size: {image.shape}, label shape: {label.shape}')\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1398,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1016, 7)"
      ]
     },
     "metadata": {},
     "execution_count": 1398
    }
   ],
   "source": [
    "# just to check if everything works we only use the sample of all data\n",
    "frac = 0.3\n",
    "# df = pd.read_csv(config.DF_PATH, usecols=['fname', 'height', 'width',\n",
    "#                                           'xmin_alb', 'ymin_alb', 'xmax_alb', 'ymax_alb',\n",
    "#                                           'label', 'kfold'])\n",
    "\n",
    "df = pd.read_csv(config.DF_PATH, usecols=['fname',\n",
    "                                          'xmin_alb', 'ymin_alb', 'xmax_alb', 'ymax_alb',\n",
    "                                          'label', 'kfold']).sample(frac=frac).reset_index(drop=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1399,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # checking for out of proportion pictures that are hard to transform and train\n",
    "# df['ratio'] = np.maximum(df.height, df.width) / np.minimum(df.height, df.width)\n",
    "# df.ratio.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1400,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dfs\n",
    "train_df = df[df.kfold.isin([0,1,2])].reset_index(drop=True)\n",
    "valid_df = df[df.kfold==3].reset_index(drop=True)\n",
    "test_df = df[df.kfold==4].reset_index(drop=True)\n",
    "\n",
    "# create dataset\n",
    "train_dataset = dataset(train_df, train_transform)\n",
    "valid_dataset = dataset(valid_df, valid_transform)\n",
    "test_dataset = dataset(test_df, test_transform)\n",
    "\n",
    "# create loaders\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1401,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dataloader test: torch.Size([32, 3, 256, 256]), torch.Size([32, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# just a check if all is good with the shapes of the loaders\n",
    "# print(f'dataloader test: {next(iter(train_loader))[0].shape}')\n",
    "print(f'dataloader test: {next(iter(train_loader))[0].shape}, {next(iter(valid_loader))[0].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1402,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in train_loader:\n",
    "#     # print(len(i))\n",
    "#     print(i[-1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1403,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1404,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # displayig the data (looks this way because of normalization)\n",
    "# batch_tensor = next(iter(train_loader))[0][:6,...]\n",
    "# grid_img = torchvision.utils.make_grid(batch_tensor, nrow=3)\n",
    "\n",
    "# # grid_img.shape\n",
    "# plt.figure(figsize=(16,6))\n",
    "# plt.imshow(grid_img.permute(1, 2, 0));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1147,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1405,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18_5().to(device) # good\n",
    "# model = models.vgg().to(device) # good\n",
    "\n",
    "# loss\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "# learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1406,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(n_epochs=1,\n",
    "                model=model,\n",
    "                train_loader=train_loader,\n",
    "                valid_loader=valid_loader,\n",
    "                criterion=criterion,\n",
    "                optimizer=optimizer,\n",
    "                lr_scheduler=lr_scheduler):\n",
    "    \n",
    "    total_time = time.time()\n",
    "    print(f'================')\n",
    "    print(f'started training...')\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        correct_on_epoch = 0 # train acc\n",
    "        total_num_images = 0 # train acc\n",
    "        epoch_loss = 0\n",
    "        epoch_iou = []\n",
    "\n",
    "        for batch, (images, labels) in enumerate(train_loader):\n",
    "            # print(f'batch: {batch}')\n",
    "\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            # print(f'image shape: {images.shape}')\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            # print(f'labels shape: {labels.shape}')\n",
    "            # print(f'labels shape: {labels.shape}')\n",
    "            # print(labels)\n",
    "\n",
    "            total_num_images += labels.size(0) # train acc\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            # print(f'outputs shape: {outputs.shape}')\n",
    "            # print(f'outputs: {outputs[0]}')\n",
    "            # print(outputs.shape)\n",
    "            # print(outputs[:-1])\n",
    "            # _, preds = torch.max(outputs, 1) # train acc\n",
    "            preds = outputs[:,-1].round() # train acc\n",
    "            # print(f'preds shape: {preds.shape}')\n",
    "            # print(f'preds: {preds}')\n",
    "\n",
    "            # print(f'outputs shape: {outputs.shape}')\n",
    "            # print(f'labels shape: {labels.shape}')\n",
    "            loss = criterion(outputs.float(), labels.float())\n",
    "            # print(f'loss: {loss}')\n",
    "            # print(f'loss shape: {loss.shape}')\n",
    "            epoch_loss += loss\n",
    "\n",
    "            # correct_on_epoch += (preds==labels).sum().item() # train acc\n",
    "            correct_on_batch = (preds==labels[:,-1]).sum().item() # train acc\n",
    "            # print(f'correct on batch: {correct_on_batch}')\n",
    "            correct_on_epoch += correct_on_batch # train acc\n",
    "\n",
    "            # batch iou\n",
    "            batch_iou = iou(labels, outputs)\n",
    "            epoch_iou.append(batch_iou)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # print(f'batch: {batch}')\n",
    "        \n",
    "        # train acc/loss\n",
    "        train_epoch_acc = round((correct_on_epoch/total_num_images), 4) # train acc\n",
    "        train_avg_epoch_loss = round(float(epoch_loss/len(train_loader)), 4)\n",
    "        # valid acc/loss\n",
    "        valid_avg_epoch_loss, valid_epoch_accuracy = test_model(model, valid_loader)\n",
    "        # valid iou\n",
    "        mean_iou = np.mean(epoch_iou)\n",
    "        \n",
    "        lr_scheduler.step()\n",
    "        epoch_time = round(time.time() - t0)\n",
    "        \n",
    "        print(f'epoch: [{epoch+1}/{n_epochs}] | train loss: {train_avg_epoch_loss} | train acc: {train_epoch_acc} | valid loss: {valid_avg_epoch_loss} | valid acc: {valid_epoch_accuracy} | iou: {mean_iou} | time: {epoch_time//60:.0f}m {epoch_time%60:.0f}s')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1407,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    correct_on_epoch = 0\n",
    "    total_num_images = 0\n",
    "    epoch_loss = 0\n",
    "\n",
    "    all_batch_acc = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for batch, (images, labels) in enumerate(test_loader):\n",
    "            \n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            \n",
    "            total_num_images += labels.size(0)\n",
    "\n",
    "            outputs = model(images)\n",
    "            # _, preds = torch.max(outputs, 1)\n",
    "            preds = outputs[:,-1].round()\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            epoch_loss += loss\n",
    "\n",
    "            # correct_on_epoch += (preds==labels).sum().item()\n",
    "            correct_on_epoch += (preds==labels[:,-1]).sum().item()\n",
    "    \n",
    "            # test_batch_accuracy = accuracy_score(labels, preds) #temp\n",
    "            # all_batch_acc.append(test_batch_accuracy) #temp\n",
    "\n",
    "    test_epoch_accuracy = round((correct_on_epoch/total_num_images), 4)\n",
    "    test_avg_epoch_loss = round(float(epoch_loss/len(test_loader)), 4)\n",
    "\n",
    "    # total_acc = round(np.mean(all_batch_acc), 4) #temp\n",
    "    \n",
    "    # print(f'total acc with acc_score: {total_acc}') #temp\n",
    "    return test_avg_epoch_loss, test_epoch_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1408,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.3333"
      ]
     },
     "metadata": {},
     "execution_count": 1408
    }
   ],
   "source": [
    "def iou(true_bb, pred_bb):\n",
    "\n",
    "    xmin_t, ymin_t, xmax_t, ymax_t, _ = true_bb\n",
    "    xmin_p, ymin_p, xmax_p, ymax_p, _ = pred_bb\n",
    "\n",
    "    xmin_intersect = np.maximum(xmin_t, xmin_p)\n",
    "    ymin_intersect = np.maximum(ymin_t, ymin_p)\n",
    "    xmax_intersect = np.minimum(xmax_t, xmax_p)\n",
    "    ymax_intersect = np.minimum(ymax_t, ymax_p)\n",
    "\n",
    "    intersection_area = (xmax_intersect - xmin_intersect) * (ymax_intersect - ymin_intersect)\n",
    "    union_area = (xmax_t - xmin_t) * (ymax_t - ymin_t) + (xmax_p - xmin_p) * (ymax_p - ymin_p) - intersection_area + 1e-6\n",
    "\n",
    "    iou = intersection_area / union_area\n",
    "\n",
    "    return round(iou, 4)\n",
    "\n",
    "bb_1 = [0, 0, 0.5, 0.5, 1]\n",
    "bb_2 = [0.25, 0, 0.75, 0.5, 1]\n",
    "\n",
    "# test\n",
    "iou(bb_1, bb_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1409,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfreeze all the params for training\n",
    "def unfreeze(model=model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1410,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "================\nstarted training...\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 5)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1406-a1b1daa6f7ec>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(n_epochs, model, train_loader, valid_loader, criterion, optimizer, lr_scheduler)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[1;31m# batch iou\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m             \u001b[0mbatch_iou\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miou\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m             \u001b[0mepoch_iou\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_iou\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1408-f7b213e20126>\u001b[0m in \u001b[0;36miou\u001b[1;34m(true_bb, pred_bb)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0miou\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrue_bb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_bb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mxmin_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mymin_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxmax_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mymax_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrue_bb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mxmin_p\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mymin_p\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxmax_p\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mymax_p\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred_bb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 5)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_model(frozen)\n",
    "unfreeze()\n",
    "train_model(unfrozen)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1363,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {'model': model,\n",
    "              'state_dict': model.state_dict()}\n",
    "\n",
    "torch.save(checkpoint, config.MODEL_OUTPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1364,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "test set acc: 0.8903\nWall time: 44.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# testing with train data\n",
    "_, train_acc = test_model(model, train_loader)\n",
    "print(f'test set acc: {train_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1365,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "test set acc: 0.9421\nWall time: 13.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# testing with test data\n",
    "_, test_acc = test_model(model, test_loader)\n",
    "print(f'test set acc: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1366,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TTA acc: 0.9163\nWall time: 1min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# tta with test data\n",
    "tta_crop = int(presize*0.9)\n",
    "tta_model = tta.ClassificationTTAWrapper(model, tta.aliases.five_crop_transform(tta_crop, tta_crop))\n",
    "\n",
    "tta_transforms = A.Compose([\n",
    "    A.SmallestMaxSize(presize),\n",
    "    A.Normalize(),\n",
    "    albumentations.pytorch.ToTensorV2()\n",
    "])\n",
    "\n",
    "tta_dataset = dataset(valid_df, transform=tta_transforms)\n",
    "tta_loader = DataLoader(tta_dataset, batch_size=1, shuffle=False) # num_workers=0 on cpu\n",
    "\n",
    "_, tta_acc = test_model(tta_model, tta_loader)\n",
    "\n",
    "print(f'TTA acc: {tta_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1367,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total time: 9m 45s\n"
     ]
    }
   ],
   "source": [
    "total_time = time.time() - start\n",
    "print(f'Total time: {total_time//60:.0f}m {total_time%60:.0f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1368,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOX_COLOR = (255, 0, 0)  # Red\n",
    "# TEXT_COLOR = (255, 255, 255)  # White\n",
    "\n",
    "\n",
    "# def visualize_bbox(image, bboxes, class_name, color=BOX_COLOR, thickness=2):\n",
    "#     \"\"\"Visualizes a single bounding box on the image\"\"\"\n",
    "#     x_min, y_min, x_max, y_max = bboxes\n",
    "#     x_min, y_min, x_max, y_max = int(x_min*256), int(y_min*256), int(x_max*256), int(y_max*256)\n",
    "\n",
    "#     cv2.rectangle(image, (x_min, y_min), (x_max, y_max),\n",
    "#                   color=color, thickness=thickness)\n",
    "\n",
    "#     ((text_width, text_height), _) = cv2.getTextSize(\n",
    "#         class_name, cv2.FONT_HERSHEY_SIMPLEX, 0.35, 1)\n",
    "#     cv2.rectangle(image, (x_min, y_min - int(1.3 * text_height)),\n",
    "#                   (x_min + text_width, y_min), BOX_COLOR, -1)\n",
    "#     cv2.putText(\n",
    "#         image,\n",
    "#         text=class_name,\n",
    "#         org=(x_min, y_min - int(0.3 * text_height)),\n",
    "#         fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "#         fontScale=0.35,\n",
    "#         color=TEXT_COLOR,\n",
    "#         lineType=cv2.LINE_AA,\n",
    "#     )\n",
    "#     return image\n",
    "\n",
    "\n",
    "# def visualize(image, bboxes, label, category_id_to_name):\n",
    "#     image = image.copy()\n",
    "#     for bboxes, category_id in zip(bboxes, label):\n",
    "#         class_name = category_id_to_name[category_id]\n",
    "#         image = visualize_bbox(image, bboxes, class_name)\n",
    "#     plt.figure(figsize=(12, 12))\n",
    "#     plt.axis('off')\n",
    "#     plt.imshow(image)\n",
    "#     # plt.show()\n",
    "\n",
    "# image = train_dataset[0][0]\n",
    "# image = np.array(image)\n",
    "# print(f'image shape: {image.shape}')\n",
    "# bbox = train_dataset[1][-1].flatten().tolist()\n",
    "# bboxes = [bbox[:-1]]  # because there is for loop for the bboxes\n",
    "# # print(f'bboxes: {bboxes}')\n",
    "# label = [int(bbox[-1])]\n",
    "# # print(f'label: {label}')\n",
    "# category_id_to_name = {1: 'cat', 0: 'dog'}\n",
    "\n",
    "# # transformed = train_transform(image=image)\n",
    "# # transformed = train_transform(image=image, bboxes=bboxes)\n",
    "\n",
    "# # visualize(\n",
    "# #     transformed['image'],\n",
    "# #     transformed['bboxes'])\n",
    "\n",
    "# visualize(image, bboxes, label, category_id_to_name)\n",
    "# # plt.imshow()"
   ]
  }
 ]
}